import lancedb
from google import genai
import json
import pandas as pd
import numpy as np
from datetime import datetime
from sentence_transformers import SentenceTransformer
from langchain_community.embeddings import HuggingFaceEmbeddings

class AiProcess:
    def __init__(self, api_key, vector_db_path):
        self.api_key = api_key
        self.vector_db_path = vector_db_path
        self.client = genai.Client(api_key=api_key)
        self.model = "gemini-embedding-001"

    # -----------------------------
    # 1) LanceDB Insert / Update
    # -----------------------------
    def lancedb_create_or_update(self, vector_db_table, doc_list, pk):
        db = lancedb.connect(self.vector_db_path)

        if vector_db_table not in db.table_names():
            tbl = db.create_table(name=vector_db_table, data=doc_list)
            print(f"[LanceDB] ì‹ ê·œ í…Œì´ë¸” ìƒì„± â†’ {vector_db_table}")
            print(f"row count: {tbl.count_rows()}")
        else:
            tbl = db.open_table(vector_db_table)
            print(f"[LanceDB] ê¸°ì¡´ í…Œì´ë¸” ì—…ë°ì´íŠ¸ â†’ {vector_db_table}")
            print(f"count before : {tbl.count_rows()}")

            tbl.merge_insert(pk) \
               .when_matched_update_all() \
               .when_not_matched_insert_all() \
               .execute(doc_list)

            print(f"count after : {tbl.count_rows()}")

        return tbl

    # -----------------------------
    # 2) Gemini Batch Embedding
    # -----------------------------
    def safe_gemini_batch_embedding(self, df):
        MAX_PAYLOAD_BYTES = 35 * 1024 * 1024

        batches = []
        current_batch = []
        current_size = 0

        # df â†’ row ë‹¨ìœ„ ë°°ì¹˜ êµ¬ì„±
        for idx, row in df.iterrows():
            text = f"{row['title']}\n{row['content']}"
            text_bytes = len(json.dumps(text).encode("utf-8"))

            if current_size + text_bytes > MAX_PAYLOAD_BYTES:
                batches.append(current_batch)
                current_batch = []
                current_size = 0

            current_batch.append((row, text))
            current_size += text_bytes

        if current_batch:
            batches.append(current_batch)

        print(f"[EMEBD BATCH NUM] : {len(batches)}")

        # ì‹¤ì œ Embedding í˜¸ì¶œ
        results = []

        for batch in batches:
            texts = [item[1] for item in batch]

            response = self.client.models.embed_content(
                model=self.model,
                contents=texts
            )

            embeddings = response.embeddings

            for i, (row, _) in enumerate(batch):
                results.append({
                    "seq": row["seq"],
                    "title": row["title"],
                    "content": row["content"],
                    "pubDate": row["pubDate"],
                    "media": row["media"],
                    "embedding": embeddings[i].values,
                    "insert_dt": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                })

        return results
    
    # -----------------------------
    # 3) similarity check
    # -----------------------------
    def news_similarity_process(self, df_new_articles: pd.DataFrame, vector_db_table: str,search_dt:str):
        """
        ìƒˆë¡œ ì„ë² ë”©ëœ ë‰´ìŠ¤ ëª©ë¡ì„ ê¸°ì¤€ìœ¼ë¡œ LanceDB ë‚´ì—ì„œ ìœ ì‚¬í•œ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
        
        Args:
            df_new_articles: ìƒˆë¡œ ì„ë² ë”©ëœ ë‰´ìŠ¤ ë°ì´í„°í”„ë ˆì„ (seq, title, content, ...)
            table_name: LanceDB í…Œì´ë¸” ì´ë¦„
            
        Returns:
            ìœ ì‚¬ë„ ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ (list of dicts)
        """
        db = lancedb.connect(self.vector_db_path)
        tbl = db.open_table(vector_db_table)

        similarity_results = []

        # ê° ë‰´ìŠ¤ embedding ê°€ì ¸ì™€ similarity search
        for _, row in df_new_articles.iterrows():
            seq = row["seq"]

            # LanceDBì—ì„œ í•´ë‹¹ seqì˜ embedding ì¡°íšŒ (ì´ë¯¸ ì´ì „ íƒœìŠ¤í¬ì—ì„œ ì €ì¥ë˜ì—ˆìœ¼ë¯€ë¡œ ì¡´ì¬í•¨)
            # 'where' ì¡°ê±´ìœ¼ë¡œ íŠ¹ì • seqë¥¼ ê²€ìƒ‰í•˜ê³ , limit(1)ë¡œ ì„ë² ë”© ë²¡í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
            query = tbl.search(None).where(f"seq = {seq}").limit(1).to_pandas()
            
            if query.empty:
                print(f"[WARN] seq={seq} embedding not found in LanceDB")
                continue
            
            emb = query.iloc[0]["embedding"]

            # top-10 ìœ ì‚¬ ë‰´ìŠ¤ ê²€ìƒ‰ (default: Euclidean distance, or Cosine/Dot Product)
            # .search(emb)ë¥¼ ì‚¬ìš©í•˜ë©´ LanceDBê°€ embì™€ ê°€ì¥ ê°€ê¹Œìš´ ë²¡í„°ë“¤ì„ ì°¾ì•„ì¤ë‹ˆë‹¤.
            topk = tbl.search(emb).limit(10)\
            .to_pandas()
                # .where(f"('pubDate' LIKE {search_dt})")\


            # ìê¸° ìì‹  ì œê±° ë° ìœ ì‚¬ë„ ì ìˆ˜ ì €ì¥
            topk = topk[topk["seq"] != seq]

            for _, r in topk.iterrows():
                similarity_results.append({
                    "base_seq": seq,
                    "cmp_seq": r["seq"],
                    # _distanceëŠ” Distance Metricì— ë”°ë¼ Cosine Distance (0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ìœ ì‚¬) ë˜ëŠ” Similarity Scoreë¥¼ ì˜ë¯¸
                    "similarity_score": float(r["_distance"]),     
                    "media": row["media"],
                })
            # print(pd.DataFrame(similarity_results))
                
        return pd.DataFrame(similarity_results).astype(str)
    
    def df_to_sql_values_string(self, df: pd.DataFrame) -> str:
        """
        DataFrame â†’ SQL VALUES ë¬¸ìì—´ ìƒì„±
        ìˆ«ìëŠ” ê·¸ëŒ€ë¡œ, ë¬¸ìì—´ì€ quotes ì²˜ë¦¬
        """
        def escape_sql(val):
            if pd.isna(val) or val is None:
                return "NULL"
            if isinstance(val, str):
                return "'" + val.replace("'", "''") + "'"  # SQL ë¬¸ìì—´ escape
            return str(val)  # ìˆ«ìëŠ” ê·¸ëŒ€ë¡œ

        values_str_list = []

        for _, row in df.iterrows():
            escaped_values = [escape_sql(row[col]) for col in df.columns]
            tuple_str = "(" + ", ".join(escaped_values) + ")"
            values_str_list.append(tuple_str)

        return ", ".join(values_str_list)
    


    # -----------------------------
    # 2) Local (ko-sentence-transformers) Batch Embedding
    # -----------------------------
    def safe_local_batch_embedding(self, df):
        MAX_BATCH_SIZE = 50
        # ëª¨ë¸ ë¡œë“œ (í•œë²ˆë§Œ ë¡œë“œ)
        embedding_model = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2"
        )
        print(f"[EMBEDDING MODEL] all-MiniLM-L6-v2")
        print(f"[INPUT SIZE] df={len(df)} rows")

        results = []

        for i in range(0, len(df), MAX_BATCH_SIZE):
            batch = df.iloc[i:i + MAX_BATCH_SIZE]

            # í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ ë§Œë“¤ê¸°
            texts = [
                f"{row['title']}\n{row['content']}"
                for _, row in batch.iterrows()
            ]

            # ğŸ”¥ HuggingFace ì„ë² ë”©
            embeddings = embedding_model.embed_documents(texts)

            # ê²°ê³¼ append
            for emb, (_, row) in zip(embeddings, batch.iterrows()):
                results.append({
                    "seq": row["seq"],
                    "title": row["title"],
                    "content": row["content"],
                    "pubDate": row["pubDate"],
                    "media": row["media"],
                    "embedding": emb,
                    "insert_dt": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                })

        print(f"[EMBED RESULT] rows={len(results)}")
        return results